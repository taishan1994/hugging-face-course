# ä½¿ç”¨Trainer APIæ¥å¾®è°ƒæ¨¡å‹

ğŸ¤— Transformers æä¾›äº†ä¸€ä¸ª Trainer ç±»æ¥å¸®åŠ©ä½ åœ¨ä½ çš„æ•°æ®é›†ä¸Šå¾®è°ƒå®ƒæä¾›çš„ä»»ä½•é¢„è®­ç»ƒæ¨¡å‹ã€‚ å®Œæˆä¸Šä¸€èŠ‚ä¸­çš„æ‰€æœ‰æ•°æ®é¢„å¤„ç†å·¥ä½œåï¼Œæ‚¨åªéœ€è¦æ‰§è¡Œå‡ ä¸ªæ­¥éª¤æ¥å®šä¹‰è®­ç»ƒå™¨ã€‚ æœ€å›°éš¾çš„éƒ¨åˆ†å¯èƒ½æ˜¯å‡†å¤‡è¿è¡Œ Trainer.train çš„ç¯å¢ƒï¼Œå› ä¸ºå®ƒåœ¨ CPU ä¸Šè¿è¡Œé€Ÿåº¦éå¸¸æ…¢ã€‚ å¦‚æœæ‚¨æ²¡æœ‰è®¾ç½® GPUï¼Œåˆ™å¯ä»¥åœ¨ Google Colab ä¸Šè®¿é—®å…è´¹çš„ GPU æˆ– TPUã€‚ 

ä¸‹é¢çš„ä»£ç ç¤ºä¾‹å‡å®šæ‚¨å·²ç»æ‰§è¡Œäº†ä¸Šä¸€èŠ‚ä¸­çš„ç¤ºä¾‹ã€‚ è¿™æ˜¯ä¸€ä¸ªç®€çŸ­çš„æ¦‚æ‹¬äº†æ‚¨éœ€è¦çš„æ‘˜è¦ï¼Œï¼š 

```python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

# è®­ç»ƒ

å®šä¹‰ Trainer ä¹‹å‰çš„ç¬¬ä¸€æ­¥æ˜¯å®šä¹‰ä¸€ä¸ª TrainingArguments ç±»ï¼Œè¯¥ç±»å°†åŒ…å« Trainer ç”¨äºè®­ç»ƒå’Œè¯„ä¼°çš„æ‰€æœ‰è¶…å‚æ•°ã€‚ æ‚¨å¿…é¡»æä¾›çš„å”¯ä¸€å‚æ•°æ˜¯ä¿å­˜è®­ç»ƒæ¨¡å‹çš„ç›®å½•ï¼Œä»¥åŠæ²¿é€”çš„æ£€æŸ¥ç‚¹ã€‚ å¯¹äºå…¶ä½™æ‰€æœ‰å†…å®¹ï¼Œæ‚¨å¯ä»¥ä¿ç•™é»˜è®¤å€¼ï¼Œè¿™å¯¹äºåŸºæœ¬å¾®è°ƒåº”è¯¥éå¸¸æœ‰æ•ˆã€‚

```python
from transformers import TrainingArguments

training_args = TrainingArguments("test-trainer")
```

ç¬¬äºŒæ­¥æ˜¯å®šä¹‰æˆ‘ä»¬çš„æ¨¡å‹ã€‚ å’Œä¸Šä¸€ç« ä¸€æ ·ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ AutoModelForSequenceClassification ç±»ï¼Œå¸¦æœ‰ä¸¤ä¸ªæ ‡ç­¾ï¼š 

```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

æ‚¨ä¼šæ³¨æ„åˆ°ï¼Œä¸ç¬¬ 3 ç« ä¸åŒçš„æ˜¯ï¼Œåœ¨å®ä¾‹åŒ–æ­¤é¢„è®­ç»ƒæ¨¡å‹åä¼šæ”¶åˆ°è­¦å‘Šã€‚ è¿™æ˜¯å› ä¸º BERT æ²¡æœ‰åœ¨å¥å­å¯¹åˆ†ç±»æ–¹é¢è¿›è¡Œè¿‡é¢„è®­ç»ƒï¼Œæ‰€ä»¥é¢„è®­ç»ƒæ¨¡å‹çš„å¤´éƒ¨å·²ç»è¢«ä¸¢å¼ƒï¼Œè€Œæ˜¯æ·»åŠ äº†ä¸€ä¸ªé€‚åˆåºåˆ—åˆ†ç±»çš„æ–°å¤´éƒ¨ã€‚ è­¦å‘Šè¡¨æ˜ä¸€äº›æƒé‡æ²¡æœ‰ä½¿ç”¨ï¼ˆå¯¹åº”äºä¸¢å¼ƒçš„é¢„è®­ç»ƒå¤´çš„é‚£äº›ï¼‰ï¼Œè€Œå…¶ä»–ä¸€äº›æƒé‡è¢«éšæœºåˆå§‹åŒ–ï¼ˆæ–°å¤´çš„é‚£äº›ï¼‰ã€‚ æœ€åé¼“åŠ±æ‚¨è®­ç»ƒæ¨¡å‹ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬ç°åœ¨è¦åšçš„ã€‚ ä¸€æ—¦æˆ‘ä»¬æœ‰äº†æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å°±å¯ä»¥å®šä¹‰ä¸€ä¸ªè®­ç»ƒå™¨ï¼Œå°†è¿„ä»Šä¸ºæ­¢æ„å»ºçš„æ‰€æœ‰å¯¹è±¡ä¼ é€’ç»™å®ƒâ€”â€”æ¨¡å‹ã€training_argsã€è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ã€æˆ‘ä»¬çš„ data_collator å’Œæˆ‘ä»¬çš„æ ‡è®°å™¨ï¼š 

```python
from transformers import Trainer

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

è¯·æ³¨æ„ï¼Œå½“æ‚¨åƒæˆ‘ä»¬åœ¨æ­¤å¤„æ‰€åšçš„é‚£æ ·ä¼ é€’æ ‡è®°å™¨æ—¶ï¼ŒTrainer ä½¿ç”¨çš„é»˜è®¤ data_collator å°†æ˜¯ä¹‹å‰å®šä¹‰çš„ DataCollatorWithPaddingï¼Œå› æ­¤æ‚¨å¯ä»¥è·³è¿‡æ­¤è°ƒç”¨ä¸­çš„ data_collator=data_collator è¡Œã€‚ åœ¨ç¬¬ 3 èŠ‚ä¸­å‘æ‚¨å±•ç¤ºè¿™éƒ¨åˆ†å¤„ç†ä»ç„¶å¾ˆé‡è¦ï¼ 

è¦åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹ï¼Œæˆ‘ä»¬åªéœ€è¦è°ƒç”¨ Trainer çš„ train æ–¹æ³•ï¼š 

```py
trainer.train()
```

è¿™å°†å¼€å§‹å¾®è°ƒï¼ˆåœ¨ GPU ä¸Šéœ€è¦å‡ åˆ†é’Ÿï¼‰å¹¶æ¯ 500 æ­¥æŠ¥å‘Šä¸€æ¬¡è®­ç»ƒæŸå¤±ã€‚ ä½†æ˜¯ï¼Œå®ƒä¸ä¼šå‘Šè¯‰æ‚¨æ¨¡å‹çš„è¡¨ç°å¦‚ä½•ï¼ˆæˆ–å·®ï¼‰ã€‚ è¿™æ˜¯å› ä¸ºï¼š 

- æˆ‘ä»¬æ²¡æœ‰é€šè¿‡å°†evaluation_strategy è®¾ç½®ä¸ºâ€œstepsâ€ï¼ˆè¯„ä¼°æ¯ä¸ªeval_stepsï¼‰æˆ–â€œepochâ€ï¼ˆåœ¨æ¯ä¸ªepoch ç»“æŸæ—¶è¯„ä¼°ï¼‰æ¥å‘Šè¯‰Trainer åœ¨è®­ç»ƒæœŸé—´è¿›è¡Œè¯„ä¼°ã€‚ 
- æˆ‘ä»¬æ²¡æœ‰ä¸º Trainer æä¾›ä¸€ä¸ª compute_metrics å‡½æ•°æ¥åœ¨æ‰€è¿°è¯„ä¼°æœŸé—´è®¡ç®—æŒ‡æ ‡ï¼ˆå¦åˆ™è¯„ä¼°åªä¼šæ‰“å°æŸå¤±ï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªéå¸¸ç›´è§‚çš„æ•°å­—ï¼‰ã€‚ 

# éªŒè¯

è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•æ„å»ºæœ‰ç”¨çš„ compute_metrics å‡½æ•°å¹¶åœ¨ä¸‹æ¬¡è®­ç»ƒæ—¶ä½¿ç”¨å®ƒã€‚ è¯¥å‡½æ•°å¿…é¡»é‡‡ç”¨ä¸€ä¸ª EvalPrediction å¯¹è±¡ï¼ˆå®ƒæ˜¯ä¸€ä¸ªå…·æœ‰é¢„æµ‹å­—æ®µå’Œ label_ids å­—æ®µçš„å‘½åå…ƒç»„ï¼‰ï¼Œå¹¶å°†è¿”å›ä¸€ä¸ªæ˜ å°„å­—ç¬¦ä¸²åˆ°æµ®ç‚¹æ•°çš„å­—å…¸ï¼ˆå­—ç¬¦ä¸²æ˜¯è¿”å›çš„åº¦é‡çš„åç§°ï¼Œè€Œæµ®ç‚¹æ•°æ˜¯å®ƒä»¬çš„å€¼ï¼‰ã€‚ è¦ä»æˆ‘ä»¬çš„æ¨¡å‹ä¸­è·å¾—ä¸€äº›é¢„æµ‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Trainer.predict å‘½ä»¤ï¼š 

```python
predictions = trainer.predict(tokenized_datasets["validation"])
print(predictions.predictions.shape, predictions.label_ids.shape)
(408, 2) (408,)
```

predict æ–¹æ³•çš„è¾“å‡ºæ˜¯å¦ä¸€ä¸ªå…·æœ‰ä¸‰ä¸ªå­—æ®µçš„å‘½åå…ƒç»„ï¼špredictionsã€label_ids å’Œ metricsã€‚ æŒ‡æ ‡å­—æ®µå°†åªåŒ…å«ä¼ é€’çš„æ•°æ®é›†çš„æŸå¤±ï¼Œä»¥åŠä¸€äº›æ—¶é—´æŒ‡æ ‡ï¼ˆé¢„æµ‹æ‰€éœ€çš„æ€»æ—¶é—´å’Œå¹³å‡æ—¶é—´ï¼‰ã€‚ ä¸€æ—¦æˆ‘ä»¬å®Œæˆäº†æˆ‘ä»¬çš„compute_metrics å‡½æ•°å¹¶å°†å…¶ä¼ é€’ç»™Trainerï¼Œè¯¥å­—æ®µä¹Ÿå°†åŒ…å«compute_metrics è¿”å›çš„æŒ‡æ ‡ã€‚ 

å¦‚æ‚¨æ‰€è§ï¼Œé¢„æµ‹æ˜¯ä¸€ä¸ªäºŒç»´æ•°ç»„ï¼Œå½¢çŠ¶ä¸º 408 x 2ï¼ˆ408 æ˜¯æˆ‘ä»¬ä½¿ç”¨çš„æ•°æ®é›†ä¸­å…ƒç´ çš„æ•°é‡ï¼‰ã€‚ è¿™äº›æ˜¯æˆ‘ä»¬ä¼ é€’ç»™é¢„æµ‹çš„æ•°æ®é›†çš„æ¯ä¸ªå…ƒç´ çš„å¯¹æ•°ï¼ˆå¦‚æ‚¨åœ¨å‰ä¸€ç« ä¸­çœ‹åˆ°çš„ï¼Œæ‰€æœ‰ Transformer æ¨¡å‹éƒ½è¿”å›å¯¹æ•°ï¼‰ã€‚ è¦å°†å®ƒä»¬è½¬æ¢ä¸ºæˆ‘ä»¬å¯ä»¥ä¸æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒçš„é¢„æµ‹ï¼Œæˆ‘ä»¬éœ€è¦åœ¨ç¬¬äºŒä¸ªè½´ä¸Šå–æœ€å¤§å€¼çš„ç´¢å¼•ï¼š 

```python
import numpy as np
preds = np.argmax(predictions.predictions, axis=-1)
```

æˆ‘ä»¬ç°åœ¨å¯ä»¥å°†è¿™äº›é¢„æµ‹ä¸æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒã€‚ ä¸ºäº†æ„å»ºæˆ‘ä»¬çš„ compute_metric å‡½æ•°ï¼Œæˆ‘ä»¬å°†ä¾èµ– ğŸ¤— Datasets åº“ä¸­çš„æŒ‡æ ‡ã€‚ æˆ‘ä»¬å¯ä»¥åƒåŠ è½½æ•°æ®é›†ä¸€æ ·è½»æ¾åŠ è½½ä¸ MRPC æ•°æ®é›†å…³è”çš„æŒ‡æ ‡ï¼Œè¿™æ¬¡æ˜¯ä½¿ç”¨ load_metric å‡½æ•°ã€‚ è¿”å›çš„å¯¹è±¡æœ‰ä¸€ä¸ªè®¡ç®—æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨æ¥è¿›è¡Œåº¦é‡è®¡ç®—ï¼š 

```python
from datasets import load_metric

metric = load_metric("glue", "mrpc")
metric.compute(predictions=preds, references=predictions.label_ids)
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

æ‚¨è·å¾—çš„ç¡®åˆ‡ç»“æœå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒï¼Œå› ä¸ºæ¨¡å‹å¤´çš„éšæœºåˆå§‹åŒ–å¯èƒ½ä¼šæ”¹å˜å®ƒå®ç°çš„æŒ‡æ ‡ã€‚ åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ä¸º 85.78%ï¼ŒF1 åˆ†æ•°ä¸º 89.97ã€‚ è¿™æ˜¯ç”¨äºè¯„ä¼° GLUE åŸºå‡†çš„ MRPC æ•°æ®é›†ç»“æœçš„ä¸¤ä¸ªæŒ‡æ ‡ã€‚ BERT è®ºæ–‡ä¸­çš„è¡¨æ ¼æŠ¥å‘Šäº†åŸºæœ¬æ¨¡å‹çš„ F1 åˆ†æ•°ä¸º 88.9ã€‚ é‚£æ˜¯uncasedæ¨¡å‹ï¼Œè€Œæˆ‘ä»¬ç›®å‰ä½¿ç”¨çš„æ˜¯casedæ¨¡å‹ï¼Œè¿™è¯´æ˜äº†æ›´å¥½çš„ç»“æœã€‚ 

å°†æ‰€æœ‰å†…å®¹åŒ…è£…åœ¨ä¸€èµ·ï¼Œæˆ‘ä»¬å¾—åˆ°äº†æˆ‘ä»¬çš„ compute_metrics å‡½æ•°ï¼š 

```python
def compute_metrics(eval_preds):
    metric = load_metric("glue", "mrpc")
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

ä¸ºäº†æŸ¥çœ‹åœ¨æ¯ä¸ªstepç»“æŸæ—¶æŠ¥å‘ŠæŒ‡æ ‡çš„å®é™…åº”ç”¨ï¼Œä¸‹é¢æ˜¯æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨è¿™ä¸ªè®¡ç®—æŒ‡æ ‡åŠŸèƒ½å®šä¹‰ä¸€ä¸ªæ–°çš„Trainerï¼š

```python
training_args = TrainingArguments("test-trainer", evaluation_strategy="epoch")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„ TrainingArgumentsï¼Œå…¶evaluation_strategy è®¾ç½®ä¸ºâ€œepochâ€å’Œä¸€ä¸ªæ–°æ¨¡å‹â€”â€”å¦åˆ™ï¼Œæˆ‘ä»¬åªä¼šç»§ç»­è®­ç»ƒæˆ‘ä»¬å·²ç»è®­ç»ƒè¿‡çš„æ¨¡å‹ã€‚ è¦å¯åŠ¨æ–°çš„è®­ç»ƒè¿è¡Œï¼Œæˆ‘ä»¬æ‰§è¡Œï¼š 

```python
trainer.train()
```

è¿™ä¸€æ¬¡ï¼Œå®ƒå°†åœ¨è®­ç»ƒæŸå¤±ä¹‹ä¸ŠæŠ¥å‘Šæ¯ä¸ª epoch ç»“æŸæ—¶çš„éªŒè¯æŸå¤±å’ŒæŒ‡æ ‡ã€‚ åŒæ ·ï¼Œç”±äºæ¨¡å‹çš„éšæœºå¤´éƒ¨åˆå§‹åŒ–ï¼Œæ‚¨è¾¾åˆ°çš„å‡†ç¡®å‡†ç¡®ç‡/F1 åˆ†æ•°å¯èƒ½ä¸æˆ‘ä»¬å‘ç°çš„ç•¥æœ‰ä¸åŒï¼Œä½†å®ƒåº”è¯¥åœ¨åŒä¸€èŒƒå›´å†…ã€‚ 

Trainer å°†åœ¨å¤šä¸ª GPU æˆ– TPU ä¸Šå¼€ç®±å³ç”¨ï¼Œå¹¶æä¾›è®¸å¤šé€‰é¡¹ï¼Œä¾‹å¦‚æ··åˆç²¾åº¦è®­ç»ƒï¼ˆåœ¨è®­ç»ƒå‚æ•°ä¸­ä½¿ç”¨ fp16 = Trueï¼‰ã€‚ æˆ‘ä»¬å°†ä»¥åè®¨è®ºå®ƒæ”¯æŒçš„æ‰€æœ‰å†…å®¹ã€‚ ä½¿ç”¨ Trainer API è¿›è¡Œå¾®è°ƒçš„ä»‹ç»åˆ°æ­¤ç»“æŸã€‚ ç¬¬ 7 ç« å°†ç»™å‡ºä¸€ä¸ªå¯¹æœ€å¸¸è§çš„ NLP ä»»åŠ¡æ‰§è¡Œæ­¤æ“ä½œçš„ç¤ºä¾‹ï¼Œä½†ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨çº¯ PyTorch ä¸­æ‰§è¡Œç›¸åŒçš„æ“ä½œã€‚ 