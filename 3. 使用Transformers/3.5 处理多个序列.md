# å¤„ç†å¤šä¸ªåºåˆ—

åœ¨ä¸Šä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†æœ€ç®€å•çš„ç”¨ä¾‹ï¼šå¯¹å•ä¸ªå°é•¿åº¦åºåˆ—è¿›è¡Œæ¨ç†ã€‚ ç„¶è€Œï¼Œä¸€äº›é—®é¢˜å·²ç»å‡ºç°ï¼š 

- æˆ‘ä»¬å¦‚ä½•å¤„ç†å¤šä¸ªåºåˆ—ï¼Ÿ 
- æˆ‘ä»¬å¦‚ä½•å¤„ç†ä¸åŒé•¿åº¦çš„å¤šä¸ªåºåˆ—ï¼Ÿ 
- è¯æ±‡ç´¢å¼•æ˜¯å”¯ä¸€èƒ½è®©æ¨¡å‹æ­£å¸¸å·¥ä½œçš„è¾“å…¥å—ï¼Ÿ 
- æœ‰æ²¡æœ‰åºåˆ—å¤ªé•¿è¿™æ ·çš„äº‹æƒ…ï¼Ÿ 

è®©æˆ‘ä»¬çœ‹çœ‹è¿™äº›é—®é¢˜ä¼šå¸¦æ¥ä»€ä¹ˆæ ·çš„é—®é¢˜ï¼Œä»¥åŠæˆ‘ä»¬å¦‚ä½•ä½¿ç”¨ ğŸ¤— Transformers API è§£å†³å®ƒä»¬ã€‚ 

# ä½¿ç”¨ä¸€æ‰¹æ•°æ®è¾“å…¥åˆ°æ¨¡å‹

åœ¨ä¸Šä¸€é¡¹ç»ƒä¹ ä¸­ï¼Œæ‚¨äº†è§£åºåˆ—å¦‚ä½•è½¬åŒ–ä¸ºæ•°å­—åˆ—è¡¨ã€‚ è®©æˆ‘ä»¬å°†æ­¤æ•°å­—åˆ—è¡¨è½¬æ¢ä¸ºTensorå¹¶å°†å…¶å‘é€åˆ°æ¨¡å‹ï¼š

```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)
# This line will fail.
model(input_ids)
```

ç»“æœï¼š

```python
IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)
```

ä¸å¥½äº†ï¼ ä¸ºä»€ä¹ˆè¿™å¤±è´¥äº†ï¼Ÿ

é—®é¢˜æ˜¯æˆ‘ä»¬å‘æ¨¡å‹å‘é€äº†å•ä¸ªåºåˆ—ï¼Œè€Œ ğŸ¤— Transformers æ¨¡å‹é»˜è®¤éœ€è¦å¤šä¸ªå¥å­ã€‚ åœ¨è¿™é‡Œï¼Œå½“æˆ‘ä»¬å°†åˆ†è¯å™¨åº”ç”¨äºä¸€ä¸ªåºåˆ—æ—¶ï¼Œæˆ‘ä»¬å°è¯•åœ¨å¹•åå®Œæˆå®ƒæ‰€åšçš„ä¸€åˆ‡ï¼Œä½†æ˜¯å¦‚æœæ‚¨ä»”ç»†è§‚å¯Ÿï¼Œæ‚¨ä¼šå‘ç°å®ƒä¸ä»…å°†è¾“å…¥ ID åˆ—è¡¨è½¬æ¢ä¸ºå¼ é‡ï¼Œè¿˜æ·»åŠ äº†ä¸€ä¸ªç»´åº¦ åœ¨å®ƒçš„ä¸Šé¢ï¼š 

```python
tokenized_inputs = tokenizer(sequence, return_tensors="pt")
print(tokenized_inputs["input_ids"])
tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
          2607,  2026,  2878,  2166,  1012,   102]])
```

è®©æˆ‘ä»¬å†è¯•ä¸€æ¬¡å¹¶æ·»åŠ æ–°çš„ç»´åº¦ï¼š

```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = torch.tensor([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```

æˆ‘ä»¬æ‰“å°è¾“å…¥IDä»¥åŠç”Ÿæˆçš„Logits - è¿™æ˜¯è¾“å‡ºï¼š

```python
Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]
Logits: [[-2.7276,  2.8789]]
```

æ‰¹å¤„ç†æ˜¯é€šè¿‡æ¨¡å‹åŒæ—¶å‘é€å¤šä¸ªå¥å­çš„è¡Œä¸ºã€‚ å¦‚æœæ‚¨åªæœ‰ä¸€å¥è¯ï¼Œæ‚¨åªéœ€ç”¨å•ä¸ªåºåˆ—æ„å»ºæ‰¹å¤„ç†ï¼š

```python
batched_ids = [ids, ids]
```

è¿™æ˜¯å«æœ‰ä¸¤ç§ç›¸åŒçš„åºåˆ—çš„æ‰¹æ¬¡ï¼

# å¡«å……è¾“å…¥

ä»¥ä¸‹åŒ…å«åˆ—è¡¨çš„åˆ—è¡¨æ— æ³•è½¬æ¢ä¸ºTensorï¼š

```python
batched_ids = [
  [200, 200, 200],
  [200, 200]
]
```

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å¡«å……ä½¿æˆ‘ä»¬çš„å¼ é‡å…·æœ‰çŸ©å½¢å½¢çŠ¶ã€‚ å¡«å……é€šè¿‡å‘å…·æœ‰è¾ƒå°‘å€¼çš„å¥å­æ·»åŠ ä¸€ä¸ªç§°ä¸ºå¡«å……æ ‡è®°çš„ç‰¹æ®Šè¯æ¥ç¡®ä¿æˆ‘ä»¬æ‰€æœ‰çš„å¥å­å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚ ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰ 10 ä¸ªåŒ…å« 10 ä¸ªå•è¯çš„å¥å­å’Œ 1 ä¸ªåŒ…å« 20 ä¸ªå•è¯çš„å¥å­ï¼Œå¡«å……å°†ç¡®ä¿æ‰€æœ‰å¥å­éƒ½æœ‰ 20 ä¸ªå•è¯ã€‚ åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œç”Ÿæˆçš„å¼ é‡å¦‚ä¸‹æ‰€ç¤ºï¼š 

```python
padding_id = 100

batched_ids = [
  [200, 200, 200],
  [200, 200, padding_id]
]
```

å¡«å……ä»¤ç‰ŒIDå¯ä»¥åœ¨Tokenizer.Pad_Token_IDä¸­æ‰¾åˆ°ã€‚ è®©æˆ‘ä»¬ä½¿ç”¨å®ƒï¼Œé€šè¿‡æ¨¡å‹åˆ†åˆ«å‘é€ä¸¤å¥è¯ï¼Œå¹¶å°†å®ƒä»¬æ‰¹å¤„ç†åœ¨ä¸€èµ·ï¼š

```python
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [[200, 200, 200], [200, 200, tokenizer.pad_token_id]]

print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)
print(model(torch.tensor(batched_ids)).logits)
tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)
tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
tensor([[ 1.5694, -1.3895],
        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)
```

æˆ‘ä»¬æˆæ‰¹é¢„æµ‹ä¸­çš„logitsæœ‰é—®é¢˜ï¼šç¬¬äºŒè¡Œåº”è¯¥ä¸ç¬¬äºŒå¥çš„logitsç›¸åŒï¼Œä½†æ˜¯æˆ‘ä»¬å¾—åˆ°äº†å®Œå…¨ä¸åŒçš„å€¼ï¼

è¿™æ˜¯å› ä¸º Transformer æ¨¡å‹çš„å…³é”®ç‰¹å¾æ˜¯å°†æ¯ä¸ªæ ‡è®°è¯­å¢ƒåŒ–çš„æ³¨æ„åŠ›å±‚ã€‚ è¿™äº›å°†è€ƒè™‘å¡«å……ä»¤ç‰Œï¼Œå› ä¸ºå®ƒä»¬æ¶‰åŠåºåˆ—çš„æ‰€æœ‰ä»¤ç‰Œã€‚ ä¸ºäº†åœ¨é€šè¿‡æ¨¡å‹ä¼ é€’ä¸åŒé•¿åº¦çš„å•ä¸ªå¥å­æˆ–ä¼ é€’å…·æœ‰ç›¸åŒå¥å­å’Œå¡«å……çš„æ‰¹æ¬¡æ—¶è·å¾—ç›¸åŒçš„ç»“æœï¼Œæˆ‘ä»¬éœ€è¦å‘Šè¯‰è¿™äº›æ³¨æ„å±‚å¿½ç•¥å¡«å……æ ‡è®°ã€‚ è¿™æ˜¯é€šè¿‡ä½¿ç”¨æ³¨æ„åŠ›æ©ç æ¥å®Œæˆçš„ã€‚ 

# æ³¨æ„åŠ›æ©ç 

attention mask æ˜¯å½¢çŠ¶ä¸è¾“å…¥ IDs tensor å®Œå…¨ç›¸åŒçš„å¼ é‡ï¼Œå¡«å…… 0s å’Œ 1sï¼š1s è¡¨ç¤ºåº”è¯¥å…³æ³¨ç›¸åº”çš„æ ‡è®°ï¼Œ0s è¡¨ç¤ºä¸åº”å…³æ³¨ç›¸åº”çš„æ ‡è®°ï¼ˆå³å®ƒä»¬åº”è¯¥è¢«æ¨¡å‹çš„æ³¨æ„åŠ›å±‚å¿½ç•¥ ï¼‰ã€‚ 

è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªæ³¨æ„åŠ›æ©ç æ¥å®Œæˆå‰é¢çš„ä¾‹å­ï¼š 

```python
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id]
]

attention_mask = [
  [1, 1, 1],
  [1, 1, 0]
]
outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
print(outputs.logits)
tensor([[ 1.5694, -1.3895],
        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
```

ç°åœ¨ï¼Œæˆ‘ä»¬ä¸ºæ‰¹å¤„ç†ä¸­çš„ç¬¬äºŒä¸ªå¥å­è·å¾—äº†ç›¸åŒçš„ logitsã€‚ 

æ³¨æ„ç¬¬äºŒä¸ªåºåˆ—çš„æœ€åä¸€ä¸ªå€¼æ˜¯ä¸€ä¸ªå¡«å…… IDï¼Œå®ƒåœ¨æ³¨æ„åŠ›æ©ç ä¸­æ˜¯ä¸€ä¸ª 0 å€¼ã€‚ 

# é•¿åºåˆ—

å¯¹äº Transformer æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥ä¼ é€’æ¨¡å‹çš„åºåˆ—é•¿åº¦æ˜¯æœ‰é™åˆ¶çš„ã€‚ å¤§å¤šæ•°æ¨¡å‹å¤„ç†å¤šè¾¾ 512 æˆ– 1024 ä¸ªæ ‡è®°çš„åºåˆ—ï¼Œå¹¶ä¸”åœ¨è¢«è¦æ±‚å¤„ç†æ›´é•¿çš„åºåˆ—æ—¶ä¼šå´©æºƒã€‚ 

è¿™ä¸ªé—®é¢˜æœ‰ä¸¤ç§è§£å†³æ–¹æ¡ˆï¼š 

- ä½¿ç”¨æ”¯æŒæ›´é•¿åºåˆ—é•¿åº¦çš„æ¨¡å‹ã€‚ 
- æˆªæ–­ä½ çš„åºåˆ—ã€‚ 

æ¨¡å‹æ”¯æŒä¸åŒçš„åºåˆ—é•¿åº¦ï¼Œæœ‰äº›æ¨¡å‹ä¸“é—¨å¤„ç†å¾ˆé•¿çš„åºåˆ—ã€‚ [Longformer](https://huggingface.co/transformers/model_doc/longformer.html) æ˜¯ä¸€ä¸ªä¾‹å­ï¼Œå¦ä¸€ä¸ªæ˜¯ [LED](https://huggingface.co/transformers/model_doc/led.html).ã€‚ å¦‚æœæ‚¨æ­£åœ¨å¤„ç†éœ€è¦å¾ˆé•¿åºåˆ—çš„ä»»åŠ¡ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨æŸ¥çœ‹è¿™äº›æ¨¡å‹ã€‚ å¦åˆ™ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨é€šè¿‡æŒ‡å®š max_sequence_length å‚æ•°æ¥æˆªæ–­åºåˆ—ï¼š 

```python
sequence = sequence[:max_sequence_length]
```

