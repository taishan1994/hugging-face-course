# æŠŠå®ƒä»¬æ”¾åœ¨ä¸€èµ·

åœ¨æœ€åå‡ èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä¸€ç›´åœ¨å°½æœ€å¤§åŠªåŠ›æ‰‹åŠ¨å®Œæˆå¤§éƒ¨åˆ†å·¥ä½œã€‚ æˆ‘ä»¬å·²ç»æ¢ç´¢äº†åˆ†è¯å™¨çš„å·¥ä½œåŸç†ï¼Œå¹¶ç ”ç©¶äº†åˆ†è¯ã€è½¬æ¢ä¸ºè¾“å…¥ IDã€å¡«å……ã€æˆªæ–­å’Œæ³¨æ„åŠ›æ©ç ã€‚ 

ç„¶è€Œï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬ 2 èŠ‚ä¸­çœ‹åˆ°çš„ï¼ŒğŸ¤— Transformers API å¯ä»¥é€šè¿‡æˆ‘ä»¬å°†åœ¨è¿™é‡Œæ·±å…¥ç ”ç©¶çš„é«˜çº§å‡½æ•°ä¸ºæˆ‘ä»¬å¤„ç†æ‰€æœ‰è¿™äº›ã€‚ å½“ä½ ç›´æ¥åœ¨å¥å­ä¸Šè°ƒç”¨ä½ çš„åˆ†è¯å™¨æ—¶ï¼Œä½ ä¼šå¾—åˆ°å‡†å¤‡é€šè¿‡ä½ çš„æ¨¡å‹çš„è¾“å…¥ï¼š

 ```python
 from transformers import AutoTokenizer
 
 checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
 tokenizer = AutoTokenizer.from_pretrained(checkpoint)
 
 sequence = "I've been waiting for a HuggingFace course my whole life."
 
 model_inputs = tokenizer(sequence)
 ```

åœ¨è¿™é‡Œï¼Œmodel_inputs å˜é‡åŒ…å«æ¨¡å‹æ­£å¸¸è¿è¡Œæ‰€éœ€çš„ä¸€åˆ‡ã€‚ å¯¹äº DistilBERTï¼Œè¿™åŒ…æ‹¬è¾“å…¥ ID å’Œæ³¨æ„åŠ›æ©ç ã€‚ æ¥å—é¢å¤–è¾“å…¥çš„å…¶ä»–æ¨¡å‹ä¹Ÿå°†å…·æœ‰åˆ†è¯å™¨å¯¹è±¡çš„è¾“å‡ºã€‚ 

æ­£å¦‚æˆ‘ä»¬å°†åœ¨ä¸‹é¢çš„ä¸€äº›ç¤ºä¾‹ä¸­çœ‹åˆ°çš„ï¼Œè¿™ç§æ–¹æ³•éå¸¸å¼ºå¤§ã€‚ é¦–å…ˆï¼Œå®ƒå¯ä»¥æ ‡è®°å•ä¸ªåºåˆ—ï¼š

```python
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

å®ƒä¸€æ¬¡ä¹Ÿå¤„ç†å¤šä¸ªåºåˆ—ï¼ŒAPIæ²¡æœ‰æ›´æ”¹ï¼š

```python
sequences = [
  "I've been waiting for a HuggingFace course my whole life.",
  "So have I!"
]

model_inputs = tokenizer(sequences)
```

å®ƒå¯ä»¥æ ¹æ®è‹¥å¹²ç›®æ ‡å¡«å……ï¼š

```python
# Will pad the sequences up to the maximum sequence length
model_inputs = tokenizer(sequences, padding="longest")

# Will pad the sequences up to the model max length
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# Will pad the sequences up to the specified max length
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```

å®ƒä¹Ÿå¯ä»¥æˆªæ–­å¥å­ï¼š

```python
sequences = [
  "I've been waiting for a HuggingFace course my whole life.",
  "So have I!"
]

# Will truncate the sequences that are longer than the model max length
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)

# Will truncate the sequences that are longer than the specified max length
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

tokenizer å¯¹è±¡å¯ä»¥å¤„ç†åˆ°ç‰¹å®šæ¡†æ¶å¼ é‡çš„è½¬æ¢ï¼Œç„¶åå¯ä»¥å°†å…¶ç›´æ¥å‘é€åˆ°æ¨¡å‹ã€‚ ä¾‹å¦‚ï¼Œåœ¨ä»¥ä¸‹ä»£ç ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬æç¤ºåˆ†è¯å™¨è¿”å›æ¥è‡ªä¸åŒæ¡†æ¶çš„å¼ é‡â€”â€”â€œptâ€è¿”å› PyTorch å¼ é‡ï¼Œâ€œtfâ€è¿”å› TensorFlow å¼ é‡ï¼Œâ€œnpâ€è¿”å› NumPy æ•°ç»„ï¼š 

```python
sequences = [
  "I've been waiting for a HuggingFace course my whole life.",
  "So have I!"
]

# Returns PyTorch tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# Returns TensorFlow tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# Returns NumPy arrays
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```

# ç‰¹æ®Šæ ‡è®°

å¦‚æœæˆ‘ä»¬æŸ¥çœ‹åˆ†è¯å™¨è¿”å›çš„è¾“å…¥ IDï¼Œæˆ‘ä»¬ä¼šå‘ç°å®ƒä»¬ä¸æˆ‘ä»¬ä¹‹å‰çš„æœ‰ä¸€ç‚¹ä¸åŒï¼š 

```python
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]
```

å¼€å¤´æ·»åŠ äº†ä¸€ä¸ªä»¤ç‰Œ IDï¼Œç»“å°¾æ·»åŠ äº†ä¸€ä¸ªã€‚ è®©æˆ‘ä»¬è§£ç ä¸Šé¢çš„ä¸¤ä¸ª ID åºåˆ—ï¼Œçœ‹çœ‹è¿™æ˜¯å…³äºä»€ä¹ˆçš„ï¼š 

```python
print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))
"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
"i've been waiting for a huggingface course my whole life."
```

åˆ†è¯å™¨åœ¨å¼€å¤´æ·»åŠ äº†ç‰¹æ®Šè¯ [CLS]ï¼Œåœ¨æœ«å°¾æ·»åŠ äº†ç‰¹æ®Šè¯ [SEP]ã€‚ è¿™æ˜¯å› ä¸ºæ¨¡å‹æ˜¯ç”¨è¿™äº›è¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œæ‰€ä»¥ä¸ºäº†è·å¾—ç›¸åŒçš„æ¨ç†ç»“æœï¼Œæˆ‘ä»¬è¿˜éœ€è¦æ·»åŠ å®ƒä»¬ã€‚ è¯·æ³¨æ„ï¼Œæœ‰äº›æ¨¡å‹ä¸æ·»åŠ ç‰¹æ®Šè¯ï¼Œæˆ–æ·»åŠ ä¸åŒçš„è¯ï¼› æ¨¡å‹ä¹Ÿå¯ä»¥ä»…åœ¨å¼€å¤´æˆ–ä»…åœ¨ç»“å°¾æ·»åŠ è¿™äº›ç‰¹æ®Šè¯ã€‚ åœ¨ä»»ä½•æƒ…å†µä¸‹ï¼Œåˆ†è¯å™¨éƒ½çŸ¥é“å“ªäº›æ˜¯é¢„æœŸçš„ï¼Œå¹¶å°†ä¸ºæ‚¨å¤„ç†ã€‚ 

# æ€»ç»“ï¼šä»åˆ†è¯å™¨åˆ°æ¨¡å‹ 

ç°åœ¨æˆ‘ä»¬å·²ç»çœ‹åˆ°äº†åˆ†è¯å™¨å¯¹è±¡åœ¨åº”ç”¨äºæ–‡æœ¬æ—¶ä½¿ç”¨çš„æ‰€æœ‰å•ç‹¬æ­¥éª¤ï¼Œè®©æˆ‘ä»¬æœ€åä¸€æ¬¡çœ‹çœ‹å®ƒå¦‚ä½•å¤„ç†å¤šä¸ªåºåˆ—ï¼ˆå¡«å……ï¼ï¼‰ã€éå¸¸é•¿çš„åºåˆ—ï¼ˆæˆªæ–­ï¼ï¼‰ä»¥åŠå¤šç§ç±»å‹çš„å¼ é‡åŠå…¶ ä¸»è¦APIï¼š 

```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
  "I've been waiting for a HuggingFace course my whole life.",
  "So have I!"
]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)
```

